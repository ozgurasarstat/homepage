<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Özgür Asar, PhD</title>
    <link>/post/</link>
    <description>Recent content in Posts on Özgür Asar, PhD</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Jul 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Robust Joint Models</title>
      <link>/post/robust_joint/</link>
      <pubDate>Wed, 17 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/robust_joint/</guid>
      <description>


&lt;div id=&#34;robust-joint-modelling-framework&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Robust Joint Modelling Framework&lt;/h2&gt;
&lt;p&gt;Joint modelling of longitudinal time-to-event outcomes typically
combines a linear mixed-effects model for repeated measures
and a Cox model with time-varying frailty for time-to-event outcome (&lt;a href=&#34;https://academic.oup.com/ije/article/44/1/334/657852&#34;&gt;Asar et al., 2015&lt;/a&gt;).
Typical distributional assumption is that random-effects and
measurement error terms in mixed-effects model are Gaussian.
However, this assumption might be restricive for real-life
problems, where it is quite likely to have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;subjects who do not
conform the population averaged trends (they are examples of
outliers in the random-effects), and&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;subjects who has a few
observations that are quite different compared to the rest
of the observations for subjects’ own collection of measurements
(they are examples of outliers in measurement error).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Gaussian distribution would not give appropriate weights to the outliers,
hence inference might be biased and inefficient, and personalised predictions might be
misleading. A natural approach would be to replace the Gaussian assumption
with t distribution. Technical details of joint models with t distributions,
and associated inferential methods are
skipped here, and interested reader is referred to &lt;a href=&#34;https://arxiv.org/abs/1905.00816&#34;&gt;Asar, Fournier and Dantan (2019)&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implementation&lt;/h2&gt;
&lt;p&gt;We describe the
R package &lt;a href=&#34;https://github.com/ozgurasarstat/robjm&#34;&gt;&lt;code&gt;robjm&lt;/code&gt;&lt;/a&gt; to implement the
joint models with Gaussian and t distributed random-effects and error terms,
and subsequently to obtain personalised dynamic predictions.
For illustration, we will use the
AIDS data-set (first 250 subjects only).
Note that the biomarker of interest is the CD4 cell counts, and the
survival event is death.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;robjm&lt;/code&gt; is still under development, hence is currently only available from Github.
Note that &lt;code&gt;robjm&lt;/code&gt; internatlly calls &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;rstan&lt;/code&gt; packages.&lt;/p&gt;
&lt;p&gt;To install &lt;code&gt;robjm&lt;/code&gt; from Github and load into the working environment,
use the following lines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#devtools::install_github(&amp;quot;ozgurasarstat/robjm&amp;quot;, quiet = TRUE)
suppressMessages(library(robjm))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;AIDS data-set can be loaded prepared for analysis using&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(aids)
data(aids.id)

aids$drug2 &amp;lt;- ifelse(aids$drug == &amp;quot;ddC&amp;quot;, 0, 1)
aids.id$drug2 &amp;lt;- ifelse(aids.id$drug == &amp;quot;ddC&amp;quot;, 0, 1)

idlist &amp;lt;- aids.id$patient

long_data &amp;lt;- aids[aids$patient %in% idlist[1:250], ]
surv_data &amp;lt;- aids.id[aids.id$patient %in% idlist[1:250], ]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Below, we first fit the joint model with Gaussian random effects and
Gaussian error terms, and then t distributed random effects and
t distributed error terms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## normal normal model
fit_nor_nor &amp;lt;- fit_jm(fixed_long = CD4 ~ obstime, 
                      random_long = ~ obstime, 
                      fixed_surv = cbind(Time, death) ~ drug2, 
                      data_long = long_data,
                      data_surv = surv_data,
                      id_long = &amp;quot;patient&amp;quot;,
                      id_surv = &amp;quot;patient&amp;quot;,
                      model = &amp;quot;nor_nor&amp;quot;,
                      timeVar = &amp;quot;obstime&amp;quot;,
                      bh = &amp;quot;weibull&amp;quot;,
                      chains = 4,
                      cores = 4,
                      iter = 2000,
                      warmup = 1000,
                      control = list(adapt_delta = 0.9)
                      )

fit_t_t &amp;lt;- fit_jm(fixed_long = CD4 ~ obstime,
                  random_long = ~ obstime,
                  fixed_surv = cbind(Time, death) ~ drug2,
                  data_long = long_data,
                  data_surv = surv_data,
                  id_long = &amp;quot;patient&amp;quot;,
                  id_surv = &amp;quot;patient&amp;quot;,
                  model = &amp;quot;t_t_mod3&amp;quot;,
                  timeVar = &amp;quot;obstime&amp;quot;,
                  bh = &amp;quot;weibull&amp;quot;,
                  chains = 4,
                  cores = 4,
                  iter = 2000,
                  warmup = 1000,
                  control = list(adapt_delta = 0.9)
                  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(fit_nor_nor$res, 
      pars = c(&amp;quot;alpha&amp;quot;, &amp;quot;Sigma&amp;quot;, &amp;quot;sigmasq&amp;quot;, &amp;quot;log_lambda&amp;quot;, 
               &amp;quot;log_nu&amp;quot;, &amp;quot;omega&amp;quot;, &amp;quot;eta&amp;quot;))
## Inference for Stan model: cc3bb0e249a9f8cb9af6f1735fffcd05.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##             mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## alpha[1]    7.62    0.02 0.31  7.01  7.40  7.62  7.83  8.20   375 1.01
## alpha[2]   -0.19    0.00 0.02 -0.24 -0.21 -0.19 -0.18 -0.15  1508 1.00
## Sigma[1,1] 22.34    0.04 2.24 18.39 20.76 22.20 23.76 27.00  2956 1.00
## Sigma[1,2]  0.09    0.00 0.11 -0.12  0.01  0.09  0.16  0.31  1162 1.00
## Sigma[2,1]  0.09    0.00 0.11 -0.12  0.01  0.09  0.16  0.31  1162 1.00
## Sigma[2,2]  0.04    0.00 0.01  0.02  0.03  0.04  0.04  0.06   348 1.01
## sigmasq     3.61    0.01 0.27  3.11  3.42  3.60  3.79  4.19  1081 1.00
## log_lambda -2.90    0.01 0.49 -3.89 -3.24 -2.91 -2.57 -1.95  2353 1.00
## log_nu      0.27    0.00 0.12  0.02  0.20  0.28  0.36  0.49  2160 1.00
## omega[1]    0.52    0.00 0.26  0.02  0.34  0.52  0.70  1.04  3523 1.00
## eta        -0.49    0.00 0.08 -0.67 -0.54 -0.48 -0.43 -0.35  1511 1.00
## 
## Samples were drawn using NUTS(diag_e) at Tue Jul 23 19:35:54 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).
 print(fit_t_t$res, 
       pars = c(&amp;quot;alpha&amp;quot;, &amp;quot;Sigma&amp;quot;, &amp;quot;sigmasq&amp;quot;, &amp;quot;phi&amp;quot;, &amp;quot;delta&amp;quot;, 
                &amp;quot;log_lambda&amp;quot;, &amp;quot;log_nu&amp;quot;, &amp;quot;omega&amp;quot;, &amp;quot;eta&amp;quot;))
## Inference for Stan model: 69f8702e79f5fb7927b6416c942566e8.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##             mean se_mean    sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## alpha[1]    7.49    0.02  0.32  6.86  7.27  7.48  7.70  8.11   386 1.01
## alpha[2]   -0.19    0.00  0.02 -0.23 -0.21 -0.19 -0.18 -0.15  1659 1.00
## Sigma[1,1] 21.69    0.09  2.31 17.41 20.07 21.60 23.17 26.48   607 1.01
## Sigma[1,2]  0.08    0.00  0.09 -0.09  0.02  0.07  0.14  0.25  1461 1.00
## Sigma[2,1]  0.08    0.00  0.09 -0.09  0.02  0.07  0.14  0.25  1461 1.00
## Sigma[2,2]  0.02    0.00  0.01  0.01  0.02  0.02  0.03  0.04   198 1.03
## sigmasq     1.43    0.01  0.22  1.04  1.28  1.42  1.58  1.90   251 1.02
## phi        40.56    2.85 23.14  9.48 21.69 35.61 54.90 94.00    66 1.08
## delta       3.14    0.04  0.52  2.31  2.75  3.09  3.45  4.28   216 1.02
## log_lambda -2.98    0.01  0.48 -3.90 -3.32 -2.98 -2.65 -2.06  3986 1.00
## log_nu      0.27    0.00  0.12  0.05  0.20  0.28  0.35  0.49  3817 1.00
## omega[1]    0.47    0.00  0.24  0.01  0.31  0.47  0.63  0.93  5890 1.00
## eta        -0.45    0.00  0.07 -0.60 -0.49 -0.44 -0.40 -0.32  2944 1.00
## 
## Samples were drawn using NUTS(diag_e) at Tue Jul 23 19:55:12 2019.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Traceplots of the MCMC samples can be visualised by&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;traceplot(fit_nor_nor$res, 
          pars = c(&amp;quot;alpha&amp;quot;, &amp;quot;Sigma&amp;quot;, &amp;quot;sigmasq&amp;quot;, &amp;quot;log_lambda&amp;quot;, 
                   &amp;quot;log_nu&amp;quot;, &amp;quot;omega&amp;quot;, &amp;quot;eta&amp;quot;),
          inc_warmup = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/robust_joint_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; traceplot(fit_t_t$res, 
           pars = c(&amp;quot;alpha&amp;quot;, &amp;quot;Sigma&amp;quot;, &amp;quot;sigmasq&amp;quot;, &amp;quot;phi&amp;quot;, &amp;quot;delta&amp;quot;, 
                    &amp;quot;log_lambda&amp;quot;, &amp;quot;log_nu&amp;quot;, &amp;quot;omega&amp;quot;, &amp;quot;eta&amp;quot;),
           inc_warmup = FALSE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/robust_joint_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;
Dynamic predictions for a new subject, say 251th subject, can be obtained by&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;newdata &amp;lt;- dplyr::filter(aids, patient == idlist[251])

fore_nor_nor &amp;lt;- predSurv_jm(object = fit_nor_nor, 
                            newdata = newdata, 
                            forecast = list(h = 5, n = 5),
                            B_control = list(nsel_b = 1, init = 0)
                            )
 
fore_t_t &amp;lt;- predSurv_jm(object = fit_t_t, 
                       newdata = newdata, 
                       forecast = list(h = 5, n = 5),
                       B_control = list(nsel_b = 1, init = 0)
                       ) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output would be displayed as a matrix by&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fore_nor_nor$output
##    id  time      mean      2.5%       50%     97.5%
## 1 251 12.67 1.0000000 1.0000000 1.0000000 1.0000000
## 2 251 13.67 0.9388233 0.8151943 0.9508994 0.9886428
## 3 251 14.67 0.8720210 0.6213313 0.8959580 0.9766481
## 4 251 15.67 0.8010925 0.4295851 0.8358410 0.9636754
## 5 251 16.67 0.7280174 0.2634484 0.7701099 0.9503350
## 6 251 17.67 0.6549468 0.1384544 0.6998950 0.9354523

fore_t_t$output
##    id  time      mean      2.5%       50%     97.5%
## 1 251 12.67 1.0000000 1.0000000 1.0000000 1.0000000
## 2 251 13.67 0.9390371 0.8574971 0.9449569 0.9810578
## 3 251 14.67 0.8736750 0.7066903 0.8856929 0.9610025
## 4 251 15.67 0.8048087 0.5550810 0.8229350 0.9391816
## 5 251 16.67 0.7336541 0.4086381 0.7558281 0.9163313
## 6 251 17.67 0.6616660 0.2788733 0.6857106 0.8928838&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
